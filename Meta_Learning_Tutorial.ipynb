{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Meta_Learning_Tutorial.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HayeonLee/Meta_Learning_Tutorial/blob/master/Meta_Learning_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGTx_OObbKFr",
        "colab_type": "text"
      },
      "source": [
        "Original Code: https://github.com/abdulfatir/prototypical-networks-tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gByYJhbkIFC",
        "colab_type": "text"
      },
      "source": [
        "# Few-shot Learning 1: Prototypical Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n8mp-NtkdX8",
        "colab_type": "text"
      },
      "source": [
        "이 튜토리얼에서 우리는 meta knowledge를 학습하여 few-shot learning을 수행하는 두가지 대표적인 네트워크, [Prototypical Network](https://arxiv.org/abs/1703.05175) (ProtoNet)과 [Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks](https://arxiv.org/abs/1703.03400) (MAML)을 배울 것입니다.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Raty31ll3_V",
        "colab_type": "text"
      },
      "source": [
        "## Few-shot Learning이란?\n",
        "(설명 추가 예정)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UytiQzugmFwS",
        "colab_type": "text"
      },
      "source": [
        "## Prototypical Network란?\n",
        "(설명 추가 예정)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ibNMiX3mQWU",
        "colab_type": "text"
      },
      "source": [
        "## ProtoNet 튜토리얼\n",
        "\n",
        "이제부터 어떻게 네트워크를 만들고 ProtoNet 학습 시킬지를 배워봅시다.\n",
        "순서는 다음과 같습니다.\n",
        "1. Tensorflow와 다른 library들을 불러온다.\n",
        "2. 데이터셋을 불러온다.\n",
        "3. 모델을 만든다.\n",
        "4. loss와 optimizer를 정의한다.\n",
        "5. Training loop를 정의한다.\n",
        "6. Training!\n",
        "7. Test\n",
        "\n",
        "그럼 하나씩 진행해보도록 합시다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "e1_Y75QXJS6h"
      },
      "source": [
        "### Import Tensorflow and other libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ql58Q8zHDxg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-9soydBCVt2",
        "colab_type": "code",
        "outputId": "5adab3e7-212b-4a79-8a4d-4d3fb7ea0556",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "#기존에 설치된 다른 버전의 tensorflow를 제거합니다.\n",
        "!pip uninstall tensorboard -y\n",
        "!pip uninstall tensorflow-gpu -y\n",
        "!pip uninstall tensorflow -y"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorboard-1.14.0:\n",
            "  Successfully uninstalled tensorboard-1.14.0\n",
            "Uninstalling tensorflow-gpu-1.14.0:\n",
            "  Successfully uninstalled tensorflow-gpu-1.14.0\n",
            "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jo0Omp5ZHWoW",
        "colab_type": "code",
        "outputId": "49d8e3e4-4fc2-4914-9387-29a287c6a8bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "source": [
        "!pip install tensorflow-gpu==1.14.0 #tensorflow gpu 버전을 설치합니다"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==1.14.0\n",
            "  Using cached https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.8.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (3.7.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.16.4)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.11.2)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.2.2)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0 (from tensorflow-gpu==1.14.0)\n",
            "  Using cached https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.14.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.33.4)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.1.7)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.14.0) (41.0.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (0.15.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (3.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.14.0) (2.8.0)\n",
            "Installing collected packages: tensorboard, tensorflow-gpu\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-gpu-1.14.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1dXdGsTwgnH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf # tensorflow를 import해줍니다"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SocdF2LhxX4R",
        "colab_type": "code",
        "outputId": "30f63a50-8842-4994-f242-bdb470dd5b65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.__version__ # 내가 사용할 tensorflow의 버전을 나타냅니다"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.14.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36vlWPtzaJMA",
        "colab_type": "code",
        "outputId": "a3d5cfc7-1392-4ca8-aab7-1d3495f718d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "if tf.test.gpu_device_name():\n",
        "  print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
        "else:\n",
        "  print(\"Please install GPU version of TF\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Default GPU Device: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Uti2MrBxZKh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 필요한 라이브러리를 import합니다.\n",
        "%matplotlib inline\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import glob\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WIHVdo7bKn-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p data/omniglot/data\n",
        "!mkdir -p data/omniglot/split\n",
        "!wget -O data/omniglot/split/trainval.txt https://github.com/kvpratama/prototypical-networks-tensorflow/blob/master/data/omniglot/splits/trainval.txt?raw=true\n",
        "!wget -O data/omniglot/split/train.txt https://github.com/kvpratama/prototypical-networks-tensorflow/blob/master/data/omniglot/splits/train.txt?raw=true\n",
        "!wget -O data/omniglot/split/test.txt https://github.com/kvpratama/prototypical-networks-tensorflow/blob/master/data/omniglot/splits/test.txt?raw=true\n",
        "!wget -O images_background.zip https://github.com/brendenlake/omniglot/blob/master/python/images_background.zip?raw=true\n",
        "!wget -O images_evaluation.zip https://github.com/brendenlake/omniglot/blob/master/python/images_evaluation.zip?raw=true\n",
        "!unzip images_background.zip -d data/omniglot/data\n",
        "!unzip images_evaluation.zip -d data/omniglot/data\n",
        "!mv data/omniglot/data/images_background/* data/omniglot/data/\n",
        "!mv data/omniglot/data/images_evaluation/* data/omniglot/data/\n",
        "!rmdir data/images_background\n",
        "!rmdir data/images_evaluation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7Z1234F7x4x",
        "colab_type": "code",
        "outputId": "4b045ef3-15f7-45fc-f349-86cfbc59b272",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls data/omniglot"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data  split\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZF1PXufnxeth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv_block(inputs, out_channels, name='conv'):\n",
        "    with tf.variable_scope(name):\n",
        "        conv = tf.layers.conv2d(inputs, out_channels, kernel_size=3, padding='SAME')\n",
        "        conv = tf.contrib.layers.batch_norm(conv, updates_collections=None, decay=0.99, scale=True, center=True)\n",
        "        conv = tf.nn.relu(conv)\n",
        "        conv = tf.contrib.layers.max_pool2d(conv, 2)\n",
        "        return conv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZM-QxRbxjb-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encoder(x, h_dim, z_dim, reuse=False):\n",
        "    with tf.variable_scope('encoder', reuse=reuse):\n",
        "        net = conv_block(x, h_dim, name='conv_1')\n",
        "        net = conv_block(net, h_dim, name='conv_2')\n",
        "        net = conv_block(net, h_dim, name='conv_3')\n",
        "        net = conv_block(net, z_dim, name='conv_4')\n",
        "        net = tf.contrib.layers.flatten(net)\n",
        "        return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sn7o4x-RxlGq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def euclidean_distance(a, b):\n",
        "    # a.shape = N x D\n",
        "    # b.shape = M x D\n",
        "    N, D = tf.shape(a)[0], tf.shape(a)[1]\n",
        "    M = tf.shape(b)[0]\n",
        "    a = tf.tile(tf.expand_dims(a, axis=1), (1, M, 1))\n",
        "    b = tf.tile(tf.expand_dims(b, axis=0), (N, 1, 1))\n",
        "    return tf.reduce_mean(tf.square(a - b), axis=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFKrv8aaxm07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_epochs = 20\n",
        "n_episodes = 100\n",
        "n_way = 60\n",
        "n_shot = 5\n",
        "n_query = 5\n",
        "n_examples = 20\n",
        "im_width, im_height, channels = 28, 28, 1\n",
        "h_dim = 64\n",
        "z_dim = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrJkolFJxpBi",
        "colab_type": "code",
        "outputId": "04a3b2c6-4c2c-4e57-ede5-1822df746f5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Load Train Dataset\n",
        "root_dir = './data/omniglot'\n",
        "train_split_path = os.path.join(root_dir, 'split', 'train.txt')\n",
        "with open(train_split_path, 'r') as train_split:\n",
        "    train_classes = [line.rstrip() for line in train_split.readlines()]\n",
        "n_classes = len(train_classes)\n",
        "train_dataset = np.zeros([n_classes, n_examples, im_height, im_width], dtype=np.float32)\n",
        "for i, tc in enumerate(train_classes):\n",
        "    alphabet, character, rotation = tc.split('/')\n",
        "    rotation = float(rotation[3:])\n",
        "    im_dir = os.path.join(root_dir, 'data', alphabet, character)\n",
        "    im_files = sorted(glob.glob(os.path.join(im_dir, '*.png')))\n",
        "    for j, im_file in enumerate(im_files):\n",
        "        im = 1. - np.array(Image.open(im_file).rotate(rotation).resize((im_width, im_height)), np.float32, copy=False)\n",
        "        train_dataset[i, j] = im\n",
        "print(train_dataset.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4112, 20, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svY5Oxd-3h6f",
        "colab_type": "code",
        "outputId": "e2120e42-e755-4896-8e75-1a3668def531",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "x = tf.placeholder(tf.float32, [None, None, im_height, im_width, channels])\n",
        "q = tf.placeholder(tf.float32, [None, None, im_height, im_width, channels])\n",
        "x_shape = tf.shape(x)\n",
        "q_shape = tf.shape(q)\n",
        "num_classes, num_support = x_shape[0], x_shape[1]\n",
        "num_queries = q_shape[1]\n",
        "y = tf.placeholder(tf.int64, [None, None])\n",
        "y_one_hot = tf.one_hot(y, depth=num_classes)\n",
        "emb_x = encoder(tf.reshape(x, [num_classes * num_support, im_height, im_width, channels]), h_dim, z_dim)\n",
        "emb_dim = tf.shape(emb_x)[-1]\n",
        "emb_x = tf.reduce_mean(tf.reshape(emb_x, [num_classes, num_support, emb_dim]), axis=1)\n",
        "emb_q = encoder(tf.reshape(q, [num_classes * num_queries, im_height, im_width, channels]), h_dim, z_dim, reuse=True)\n",
        "dists = euclidean_distance(emb_q, emb_x)\n",
        "log_p_y = tf.reshape(tf.nn.log_softmax(-dists), [num_classes, num_queries, -1])\n",
        "ce_loss = -tf.reduce_mean(tf.reshape(tf.reduce_sum(tf.multiply(y_one_hot, log_p_y), axis=-1), [-1]))\n",
        "acc = tf.reduce_mean(tf.to_float(tf.equal(tf.argmax(log_p_y, axis=-1), y)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0707 12:29:44.478682 139880249501568 deprecation.py:323] From <ipython-input-4-484dc49fabd8>:3: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "W0707 12:29:44.486674 139880249501568 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0707 12:29:47.129935 139880249501568 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0707 12:29:47.457491 139880249501568 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "W0707 12:29:47.965467 139880249501568 deprecation.py:323] From <ipython-input-15-225e7779b1b3>:16: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_n-0LrbBxurt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_op = tf.train.AdamOptimizer().minimize(ce_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4bc4V2X04N9",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5v9ZZq8zx7ex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess = tf.InteractiveSession()\n",
        "init_op = tf.global_variables_initializer()\n",
        "sess.run(init_op)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvGMxVEj9Y7z",
        "colab_type": "code",
        "outputId": "87d0ae69-62f3-4482-b524-0a6d290c6515",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        }
      },
      "source": [
        "for ep in range(n_epochs):\n",
        "    for epi in range(n_episodes):\n",
        "        epi_classes = np.random.permutation(n_classes)[:n_way]\n",
        "        support = np.zeros([n_way, n_shot, im_height, im_width], dtype=np.float32)\n",
        "        query = np.zeros([n_way, n_query, im_height, im_width], dtype=np.float32)\n",
        "        for i, epi_cls in enumerate(epi_classes):\n",
        "            selected = np.random.permutation(n_examples)[:n_shot + n_query]\n",
        "            support[i] = train_dataset[epi_cls, selected[:n_shot]]\n",
        "            query[i] = train_dataset[epi_cls, selected[n_shot:]]\n",
        "        support = np.expand_dims(support, axis=-1)\n",
        "        query = np.expand_dims(query, axis=-1)\n",
        "        labels = np.tile(np.arange(n_way)[:, np.newaxis], (1, n_query)).astype(np.uint8)\n",
        "        _, ls, ac = sess.run([train_op, ce_loss, acc], feed_dict={x: support, q: query, y:labels})\n",
        "        if (epi+1) % 50 == 0:\n",
        "            print('[epoch {}/{}, episode {}/{}] => loss: {:.5f}, acc: {:.5f}'.format(ep+1, n_epochs, epi+1, n_episodes, ls, ac))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[epoch 1/20, episode 50/100] => loss: 2.09691, acc: 0.68000\n",
            "[epoch 1/20, episode 100/100] => loss: 1.40069, acc: 0.81667\n",
            "[epoch 2/20, episode 50/100] => loss: 1.10882, acc: 0.85667\n",
            "[epoch 2/20, episode 100/100] => loss: 0.94011, acc: 0.84333\n",
            "[epoch 3/20, episode 50/100] => loss: 0.63034, acc: 0.91333\n",
            "[epoch 3/20, episode 100/100] => loss: 0.49404, acc: 0.93000\n",
            "[epoch 4/20, episode 50/100] => loss: 0.42604, acc: 0.92667\n",
            "[epoch 4/20, episode 100/100] => loss: 0.35078, acc: 0.96333\n",
            "[epoch 5/20, episode 50/100] => loss: 0.34698, acc: 0.95000\n",
            "[epoch 5/20, episode 100/100] => loss: 0.27430, acc: 0.97000\n",
            "[epoch 6/20, episode 50/100] => loss: 0.26539, acc: 0.95667\n",
            "[epoch 6/20, episode 100/100] => loss: 0.20937, acc: 0.96667\n",
            "[epoch 7/20, episode 50/100] => loss: 0.17970, acc: 0.97333\n",
            "[epoch 7/20, episode 100/100] => loss: 0.17806, acc: 0.98000\n",
            "[epoch 8/20, episode 50/100] => loss: 0.14741, acc: 0.98333\n",
            "[epoch 8/20, episode 100/100] => loss: 0.19914, acc: 0.96667\n",
            "[epoch 9/20, episode 50/100] => loss: 0.18217, acc: 0.97333\n",
            "[epoch 9/20, episode 100/100] => loss: 0.19450, acc: 0.95333\n",
            "[epoch 10/20, episode 50/100] => loss: 0.19492, acc: 0.95000\n",
            "[epoch 10/20, episode 100/100] => loss: 0.10824, acc: 0.98000\n",
            "[epoch 11/20, episode 50/100] => loss: 0.17206, acc: 0.96000\n",
            "[epoch 11/20, episode 100/100] => loss: 0.12593, acc: 0.97333\n",
            "[epoch 12/20, episode 50/100] => loss: 0.12870, acc: 0.96333\n",
            "[epoch 12/20, episode 100/100] => loss: 0.10097, acc: 0.97333\n",
            "[epoch 13/20, episode 50/100] => loss: 0.15451, acc: 0.97667\n",
            "[epoch 13/20, episode 100/100] => loss: 0.06234, acc: 0.98333\n",
            "[epoch 14/20, episode 50/100] => loss: 0.08252, acc: 0.99000\n",
            "[epoch 14/20, episode 100/100] => loss: 0.16847, acc: 0.97000\n",
            "[epoch 15/20, episode 50/100] => loss: 0.07193, acc: 0.99000\n",
            "[epoch 15/20, episode 100/100] => loss: 0.09566, acc: 0.97667\n",
            "[epoch 16/20, episode 50/100] => loss: 0.08881, acc: 0.97667\n",
            "[epoch 16/20, episode 100/100] => loss: 0.10484, acc: 0.96667\n",
            "[epoch 17/20, episode 50/100] => loss: 0.07943, acc: 0.97667\n",
            "[epoch 17/20, episode 100/100] => loss: 0.06895, acc: 0.98667\n",
            "[epoch 18/20, episode 50/100] => loss: 0.07089, acc: 0.98667\n",
            "[epoch 18/20, episode 100/100] => loss: 0.08318, acc: 0.97333\n",
            "[epoch 19/20, episode 50/100] => loss: 0.16690, acc: 0.96667\n",
            "[epoch 19/20, episode 100/100] => loss: 0.11464, acc: 0.97667\n",
            "[epoch 20/20, episode 50/100] => loss: 0.12412, acc: 0.96000\n",
            "[epoch 20/20, episode 100/100] => loss: 0.08786, acc: 0.96667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVXaYPolYYN5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}